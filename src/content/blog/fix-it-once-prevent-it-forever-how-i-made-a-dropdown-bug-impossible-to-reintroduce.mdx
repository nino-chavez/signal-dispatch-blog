---
title: "Fix It Once, Prevent It Forever: How I Made a Dropdown Bug Impossible to Reintroduce"
publishedAt: "2025-07-29T05:16:00.000Z"
author: "Nino Chavez"
excerpt: "This one started like a lot of bugs do: Dropdowns werenâ€™t updating after a bulk import. The data was there, the page reloaded, but the dropdowns stayed stale. Console warnings, frustrated..."
category: "AI & Automation"
tags: ["architecture"]
featured: false
source: "linkedin"
linkedinUrl: "https://www.linkedin.com/pulse/fix-once-prevent-forever-how-i-made-dropdown-bug-nino-chavez-04yyc"
---

This one started like a lot of bugs do: Dropdowns werenâ€™t updating after a bulk import. The data was there, the page reloaded, but the dropdowns stayed stale. Console warnings, frustrated debugging, and that subtle, familiar itchâ€”"Something here isnâ€™t wired right."

Eventually, I traced it to a **cache invalidation mismatch**.

-   One component (BulkTeamUploader) was correctly invalidating the React Query cache.
    
-   Another (AssignTeams) was bypassing it entirely, calling Supabase directly.
    

They werenâ€™t speaking the same languageâ€”and the result was inconsistent state and broken UX.

* * *

### The Fix: Not Just Patch, But Protect

I couldâ€™ve patched it by syncing the fetch call. But I didnâ€™t want to see this kind of bug againâ€”especially as I scale this project and rely more heavily on AI-generated components.

So I built a **multi-layer safeguard system**.

* * *

### ðŸ” Root Cause: Mixed Data Fetching Patterns

**Fixed** by switching to approved edge function queries and wrapping everything in the shared caching strategy.

### ðŸ§¯ Console Errors: React Hooks and Missing States

**Fixed** by adding loading states, dependency arrays, and proper error handling.

* * *

### ðŸ” New Prevention Layers

These werenâ€™t just fixesâ€”theyâ€™re now **enforcements**, baked into the system:

### 1\. Custom ESLint Rules

I wrote an internal plugin that includes:

-   no-direct-supabase-in-components
    
-   require-approved-queries
    
-   cache-invalidation-consistency
    

These throw **build-breaking errors** if violated.

### 2\. Automated Validation Script

A custom TypeScript script that:

-   Scans for mismatched fetch/cache patterns
    
-   Reports violations with fix suggestions
    
-   Runs in pre-commit, and can be triggered manually
    

### 3\. Updated Dev Scripts

In package.json:

-   npm run validate-data-fetching
    
-   npm run pre-component-work Tied into lint-staged to catch issues before they land.
    

### 4\. Documentation for Myself (and AI)

Everything is now codified in a working doc: docs/development/data-fetching-patterns.md Includes approved query patterns, troubleshooting, and reusable templates for future component generation.

* * *

### ðŸ§± Why It Matters (Especially with AI in the Mix)

Iâ€™m relying more and more on AI to scaffold components, write boilerplate, and even generate workflows. But with that speed comes riskâ€”especially if the AI doesnâ€™t â€œrememberâ€ the architectural rules.

By creating **automated guardrails**, Iâ€™ve made it harder for myself (or a co-pilot) to unknowingly break core patterns.

And yeahâ€”I tested it. You literally *canâ€™t* reintroduce this bug without triggering a lint error, failing a pre-commit, or breaking the build.

* * *

### âœ… Result

-   Dropdowns now populate immediately after a bulk import
    
-   Console logs are clean
    
-   Future bugs in this category are blocked at the gates
    
-   My AI dev loop is safer and more predictable
    
-   I trust the system more
    

* * *

### Final Thought

This wasnâ€™t just about fixing a dropdown. It was a reminder that **AI wonâ€™t catch architectural driftâ€”unless you teach it how**.

What Iâ€™m building here isnâ€™t new in spirit. Weâ€™ve all used style guides, linters, static analyzers, CI pipelines with type checks and test coverage thresholds. But now the code is moving fasterâ€”because weâ€™re not always the ones writing it.

AI can scaffold things in seconds, but it doesnâ€™t carry your system context, your constraints, or your edge casesâ€”unless you **embed that context back into the workflow**.

So thatâ€™s what Iâ€™m doing. Codifying patterns. Enforcing them. Turning architectural decisions into **lint rules**, **scripts**, and **guardrails** that even an AI co-pilot has to obey.

Where does this go? Probably toward the same place all software quality tools have gone:

-   Plugins and validators for AI agents
    
-   Contracts between code generators and build systems
    
-   Observability for logic and architecture, not just logs and metrics
    
-   New layers of static + dynamic analysis, purpose-built for AI-written code
    

The thing is, we **havenâ€™t figured out how to trust AI code yet**. Not because it wonâ€™t compileâ€”but because it skips the part where we sweat the details.

So for now, we build our own safety rails. And maybe someone out there is already working on making that trust layer a product.

I hope so. Because Iâ€™d use it tomorrow.

> Real-world screenshot of my VSCode IDE.

![](https://media.licdn.com/dms/image/v2/D5612AQGmQGUwtlYogA/article-inline_image-shrink_1500_2232/B56ZhTg8eeG4AU-/0/1753747803365?e=1762387200&v=beta&t=1mBCUMUKAKQ-5FxVHE8gOuvDABljL6FX05Rqwcf3Htc)